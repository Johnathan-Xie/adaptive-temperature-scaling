{
  "calibration_type": "transformer",
  "in_features": 5120,
  "intermediate_size": 13824,
  "max_position_embeddings": 5120,
  "num_attention_heads": 40,
  "layer_idx": 40,
  "attention_dropout": 0.0,
  "num_key_value_heads": 40,
  "feature_key": "hidden_states",
  "freeze_base_model": true,
  "inference_mode": true,
  "init_temperature": 1.0,
  "task_type": "CAUSAL_LM"
}